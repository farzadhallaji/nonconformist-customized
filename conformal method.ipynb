{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library ðŸ˜š\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "# from IPython.display import Image  \n",
    "from IPython.display import Image, display\n",
    "from sklearn.tree import export_graphviz\n",
    "# import pydotplus\n",
    "\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config ðŸ˜š\n",
    "path_to_datasets = '/home/farzad/Desktop/semiWithTree/originDataset/'\n",
    "dataset_name = 'bupa'\n",
    "\n",
    "\n",
    "dataset_path  = path_to_datasets + dataset_name\n",
    "base_classifier = DecisionTreeClassifier\n",
    "random_state = 0\n",
    "min_samples_leaf=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_xy(test_data):\n",
    "    # assert : class = last atr ðŸ˜š\n",
    "    x_test = test_data.values[:, 0:-1]\n",
    "    y_test = (test_data.values[:, -1]).astype('int')\n",
    "    \n",
    "    return x_test,y_test\n",
    "\n",
    "def read_data(dataset_path) :\n",
    "    \n",
    "    train_raw_data = loadarff(dataset_path+'/train.arff')\n",
    "    test_raw_data = loadarff(dataset_path+'/test.arff')\n",
    "    \n",
    "    train_data = pd.DataFrame(train_raw_data[0])\n",
    "    test_data = pd.DataFrame(test_raw_data[0])\n",
    "    \n",
    "    train_data['Class'] = train_data['Class'].astype(int)\n",
    "    test_data['Class'] = test_data['Class'].astype(int)\n",
    "    \n",
    "    return train_data,test_data\n",
    "\n",
    "\n",
    "def get_rate_p(train_y) : \n",
    "    \n",
    "    counter=collections.Counter(train_y)\n",
    "    tuple_list_pn = counter.most_common()\n",
    "    \n",
    "    return tuple_list_pn[0][1]/(tuple_list_pn[0][1]+tuple_list_pn[1][1]) , tuple_list_pn\n",
    "\n",
    "def split_trainset(train_data) :\n",
    "    \n",
    "    labeled , unlabeled = [],[]\n",
    "    \n",
    "    size_dataset = len(train_data)\n",
    "    train_x,train_y = divide_xy(train_data)\n",
    "    \n",
    "    rate_p , tuple_list_pn = get_rate_p(train_y)\n",
    "    \n",
    "    size_labeled_data = round(0.1 * size_dataset)\n",
    "    size_unlabeled_data = size_dataset - size_labeled_data\n",
    "    \n",
    "    size_labeled_p_data = round(rate_p*size_labeled_data)\n",
    "    size_labeled_n_data = size_labeled_data - size_labeled_p_data\n",
    "    \n",
    "    labeled_index = []\n",
    "    unlabeled_index = []\n",
    "    selected_pl = 0\n",
    "    selected_nl = 0\n",
    "    \n",
    "    for i,cls in enumerate(train_y):\n",
    "        # if data point class's == 0 ðŸ˜š\n",
    "        if cls == tuple_list_pn[0][0] :\n",
    "            if selected_pl < size_labeled_p_data :\n",
    "                labeled_index.append(i)\n",
    "                selected_pl+=1\n",
    "            else :\n",
    "                unlabeled_index.append(i)\n",
    "        else :\n",
    "            if selected_nl < size_labeled_n_data :\n",
    "                labeled_index.append(i)\n",
    "                selected_nl+=1\n",
    "            else :\n",
    "                unlabeled_index.append(i)\n",
    "                \n",
    "    for i in labeled_index:\n",
    "        labeled.append(train_data.values[i])\n",
    "    \n",
    "    for i in unlabeled_index:\n",
    "        unlabeled.append(train_data.values[i])\n",
    "    \n",
    "#     print(size_dataset , size_labeled_data , size_unlabeled_data)\n",
    "#     print(rate_p , tuple_list_pn)\n",
    "#     print(size_labeled_p_data , size_labeled_n_data)\n",
    "#     print(selected_pl/(selected_pl+selected_nl),selected_pl, selected_nl)\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(labeled,columns=train_data.columns),pd.DataFrame(unlabeled,columns=train_data.columns),rate_p,tuple_list_pn\n",
    "\n",
    "def evaluate_classifier(base_classifier, labeled_data, test_data):\n",
    "    \n",
    "    labeled_x,labeled_y = divide_xy(labeled_data)\n",
    "    test_x,test_y = divide_xy(test_data)\n",
    "\n",
    "    # dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "    dtree=base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf)\n",
    "    dtree.fit(labeled_x,labeled_y)\n",
    "    \n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,filled=True, rounded=True,special_characters=True)\n",
    "#     graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#     img = Image(graph.create_png())\n",
    "    img=None\n",
    "    y_pred = dtree.predict(test_x)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(test_y, y_pred)\n",
    "    \n",
    "    return accuracy , img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonconformist.base import ClassifierAdapter\n",
    "from nonconformist.cp import TcpClassifier\n",
    "from nonconformist.nc import ClassifierNc, MarginErrFunc\n",
    "\n",
    "def confidency(name , DTclassifier , labeled_data , unlabeled_data , i , confidence) :\n",
    "\n",
    "    lbl = None\n",
    "    test_x,test_y = divide_xy(unlabeled_data)\n",
    "    train_x,train_y = divide_xy(labeled_data)\n",
    "    is_confident = False\n",
    "    if name == 'Probability' :\n",
    "        lbl = DTclassifier.predict([test_x[i]])\n",
    "        i_confidence = DTclassifier.predict_proba([test_x[i]])\n",
    "        if max(i_confidence[0]) > confidence :\n",
    "            is_confident = True\n",
    "            \n",
    "            \n",
    "    elif name == 'tcp' :\n",
    "        model = ClassifierAdapter(base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf))\n",
    "        nc = ClassifierNc(model, MarginErrFunc())\n",
    "        tcp = TcpClassifier(nc)\n",
    "        tcp.fit(train_x,train_y)\n",
    "        \n",
    "        prediction_conf = tcp.predict_conf(test_x[[i], :])\n",
    "        lbl = [prediction_conf[0][0]]\n",
    "        \n",
    "        \n",
    "        ss=0\n",
    "        me=0\n",
    "        for i in range(50) :\n",
    "            prediction = tcp.predict(test_x[[i], :])\n",
    "            me += abs(prediction[0][0]-prediction[0][1]) * max(prediction[0][0],prediction[0][1])\n",
    "\n",
    "            prediction = tcp.predict_conf(test_x[[i], :])\n",
    "            ss += prediction[0][1]*prediction[0][2]\n",
    "        print(ss-me , end=\" , \")\n",
    "        \n",
    "        if ss-me > confidence :\n",
    "            is_confident = True\n",
    "        \n",
    "    return is_confident , lbl\n",
    "\n",
    "\n",
    "def selection_metric(labeled_data,unlabeled_data ,rate_p,tuple_list_pn , confidence,selection_rate , confidence_method_name) :\n",
    "    \n",
    "    labeled_x,labeled_y = divide_xy(labeled_data)\n",
    "    unlabeled_x,unlabeled_y = divide_xy(unlabeled_data)\n",
    "\n",
    "    # dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "    DTclassifier = base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf)\n",
    "    DTclassifier.fit(labeled_x,labeled_y)\n",
    "    \n",
    "    removed_selected_data = unlabeled_data.copy()\n",
    "    total_selected_labeling = pd.DataFrame(columns=labeled_data.columns)\n",
    "    selected_labeling = pd.DataFrame(columns=labeled_data.columns)\n",
    "    selected_index = []\n",
    "    selected_y = []\n",
    "    \n",
    "    \n",
    "    for i  in range(len(removed_selected_data)) :\n",
    "        is_confident , lbl = confidency(confidence_method_name , DTclassifier , labeled_data ,\n",
    "                                        removed_selected_data , i , confidence)\n",
    "        if is_confident: \n",
    "            selected_index.append(i)\n",
    "            selected_y.append(lbl[0])\n",
    "            # set class\n",
    "            removed_selected_data.at[i, 'Class'] = lbl[0]\n",
    "            \n",
    "    \n",
    "    selected_index_p = []\n",
    "    selected_index_n = []\n",
    "    \n",
    "    \n",
    "    size_selected  = round(selection_rate * len(labeled_data))\n",
    "    print(' PISH FARZ  size_selected : ', size_selected)\n",
    "    \n",
    "    #should be constant rate \n",
    "    new_rate_p,new_tuple_list_pn = get_rate_p(np.array(selected_y))\n",
    "        \n",
    "    len_new_selected_p = new_tuple_list_pn[0][1]\n",
    "    len_new_selected_n = new_tuple_list_pn[1][1]\n",
    "    \n",
    "    len_lebeled_p = tuple_list_pn[0][1]\n",
    "    len_lebeled_n = tuple_list_pn[1][1]\n",
    "\n",
    "    size_select_p = 0\n",
    "    size_select_n = 0\n",
    "    \n",
    "    \n",
    "    print('rate_p:',rate_p , '  new_rate_p:',new_rate_p)\n",
    "    print('tuple_list_pn:',tuple_list_pn , '  new_tuple_list_pn:',new_tuple_list_pn)\n",
    "    \n",
    "    if new_rate_p > rate_p :\n",
    "        size_select_n = round(min(len_new_selected_n , size_selected * (1-rate_p)))\n",
    "        size_select_p = round(size_select_n * (rate_p/(1-rate_p)))\n",
    "        size_selected = size_select_n + size_select_p\n",
    "            \n",
    "    else :\n",
    "        size_select_p = round(min(len_new_selected_p , size_selected * rate_p))\n",
    "        size_select_n = round(size_select_p * ((1-rate_p)/rate_p))\n",
    "        size_selected = size_select_n + size_select_p\n",
    "\n",
    "\n",
    "#     size_select_p = len_new_selected_p\n",
    "#     size_select_n = len_new_selected_n\n",
    "#     size_selected = size_select_n + size_select_p\n",
    "            \n",
    "        \n",
    "    print('size_select_p : ' , size_select_p , '   size_select_n : ' , size_select_n, '   size_selected : ' , size_selected)\n",
    "    \n",
    "    \n",
    "    p = new_tuple_list_pn[0][0]\n",
    "    \n",
    "    i=0\n",
    "    while(size_select_p > 0):\n",
    "        if selected_y[i] == p :\n",
    "            selected_index_p.append(i)      \n",
    "            size_select_p-=1\n",
    "        i+=1\n",
    "                \n",
    "    i=0\n",
    "    while(size_select_n > 0):\n",
    "        if selected_y[i] != p :\n",
    "            selected_index_n.append(i)  \n",
    "            size_select_n-=1\n",
    "        i+=1\n",
    "        \n",
    "    print('selected_index_p : ',len(selected_index_p))\n",
    "    print('selected_index_n : ',len(selected_index_n))\n",
    "    \n",
    "    \n",
    "    for i in range(len(selected_index_p)):\n",
    "        selected_labeling=selected_labeling.append(removed_selected_data.iloc[selected_index_p[i]] ,ignore_index=True)\n",
    "    print('selected_labeling_p: ', len(selected_labeling))\n",
    "        \n",
    "    for i in range(len(selected_index_n)):\n",
    "        selected_labeling=selected_labeling.append(removed_selected_data.iloc[selected_index_n[i]] ,ignore_index=True)\n",
    "    print('selected_labeling_n: ', len(selected_labeling))\n",
    "\n",
    "    removed_selected_data.drop(removed_selected_data.index[selected_index])\n",
    "    \n",
    "    total_selected_labeling = pd.concat([labeled_data ,selected_labeling],ignore_index=True)\n",
    "    \n",
    "    return total_selected_labeling,removed_selected_data\n",
    "\n",
    "\n",
    "def self_labeling(labeled_data , unlabeled_data , iteration , rate_p,tuple_list_pn , confidence,selection_rate,confidence_method_name):\n",
    "\n",
    "    \n",
    "    labeled_unlabel_data = labeled_data.copy()\n",
    "    removed_selected_data = unlabeled_data.copy()\n",
    "    \n",
    "    while iteration:\n",
    "        \n",
    "        selected_labeling,removed_selected_data = selection_metric(labeled_unlabel_data,removed_selected_data,\n",
    "                                                                   rate_p,tuple_list_pn ,\n",
    "                                                                   confidence,selection_rate,\n",
    "                                                                   confidence_method_name)\n",
    "        labeled_unlabel_data = pd.concat([labeled_unlabel_data , selected_labeling] , ignore_index=True)\n",
    "        \n",
    "        print('iteration:' , iteration , ' , selected_labeling:' , len(selected_labeling)\n",
    "             , ' , labeled_data:' , len(labeled_data))\n",
    "        iteration-=1\n",
    "        \n",
    "    return labeled_unlabel_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_data,test_data = read_data(dataset_path)\n",
    "# train_x,train_y = divide_xy(train_data)\n",
    "# test_x , test_y = divide_xy(test_data)\n",
    "# train_data['Class'] = train_data['Class']-1\n",
    "# test_data['Class'] = test_data['Class']-1\n",
    "\n",
    "# labeled_data,unlabeled_data , rate_p,tuple_list_pn = split_trainset(train_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a1 , img1 = evaluate_classifier(base_classifier , labeled_data  , test_data)\n",
    "# a1\n",
    "# total_labeled_data = self_labeling(labeled_data , unlabeled_data , 1, \n",
    "#                                    rate_p,tuple_list_pn ,\n",
    "#                                    confidence=10,selection_rate = 1,confidence_method_name='tcp')\n",
    "# total_labeled_data\n",
    "# # a2 , img2 = evaluate_classifier(base_classifier , total_labeled_data  , test_data)\n",
    "\n",
    "\n",
    "# # # display(img1)\n",
    "# # # display(img2)\n",
    "# # print(a1 , a2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nonconformist.cp import TcpClassifier\n",
    "# from nonconformist.nc import NcFactory\n",
    "\n",
    "\n",
    "# model = base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf)\t# Create the underlying model\n",
    "# nc = NcFactory.create_nc(model)\t# Create a default nonconformity function\n",
    "# tcp = TcpClassifier(nc)\t\t\t# Create a transductive conformal classifier\n",
    "\n",
    "# tcp.fit(train_x,train_y)\n",
    "# prediction = tcp.predict(test_x[[1], :], significance=0.9)\n",
    "# prediction = tcp.predict_conf(test_x[[1], :])\n",
    "\n",
    "# prediction\n",
    "\n",
    "# # test_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  0.6106194690265486\n",
      "accuracy  0.6814159292035398 33\n"
     ]
    }
   ],
   "source": [
    "from nonconformist.base import ClassifierAdapter\n",
    "from nonconformist.cp import TcpClassifier\n",
    "from nonconformist.nc import ClassifierNc, MarginErrFunc\n",
    "\n",
    "\n",
    "train_data,test_data = read_data(dataset_path)\n",
    "\n",
    "train_data['Class'] = train_data['Class']-1\n",
    "test_data['Class'] = test_data['Class']-1\n",
    "\n",
    "train_x,train_y = divide_xy(train_data)\n",
    "test_x , test_y = divide_xy(test_data)\n",
    "\n",
    "labeled_data,unlabeled_data , rate_p,tuple_list_pn = split_trainset(train_data)\n",
    "labeled_x,labeled_y = divide_xy(labeled_data)\n",
    "unlabeled_x,unlabeled_y = divide_xy(unlabeled_data)\n",
    "\n",
    "\n",
    "dtree = base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf)\n",
    "dtree.fit(labeled_x,labeled_y)\n",
    "y_pred = dtree.predict(test_x)\n",
    "accuracy0 = metrics.accuracy_score(test_y, y_pred)\n",
    "print('accuracy ' , accuracy0)\n",
    "\n",
    "model = ClassifierAdapter(base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf))\n",
    "nc = ClassifierNc(model, MarginErrFunc())\n",
    "tcp = TcpClassifier(nc)\n",
    "tcp.fit(labeled_x,labeled_y)\n",
    "\n",
    "\n",
    "\n",
    "unlabeled_data_cp = unlabeled_data.copy()\n",
    "labeled_data_cp = labeled_data.copy()\n",
    "\n",
    "p = tcp.predict_conf(unlabeled_x)\n",
    "for i,ipre in enumerate(p):\n",
    "    label, confidence, credibility = ipre\n",
    "    if credibility > 0.7 and confidence > 0.8 :\n",
    "        unlabeled_data_cp['Class'][i] = label\n",
    "        labeled_data_cp = labeled_data_cp.append(unlabeled_data_cp.iloc[i])\n",
    "        \n",
    "train_x,train_y = divide_xy(labeled_data_cp)\n",
    "dtree = base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf)\n",
    "dtree.fit(train_x,train_y)\n",
    "y_pred = dtree.predict(test_x)\n",
    "accuracy1 = metrics.accuracy_score(test_y, y_pred)\n",
    "print('accuracy ' , accuracy1 , len(labeled_data_cp))\n",
    "\n",
    "# accuracy = metrics.accuracy_score(test_y, y_pred)\n",
    "\n",
    "# for sss in range(len(test_x)):\n",
    "# #     sss = \n",
    "\n",
    "#     s0=0\n",
    "#     s1=0\n",
    "#     me=0\n",
    "#     p1=0\n",
    "#     p0=0\n",
    "#     for i in range(100) :\n",
    "#         prediction = tcp.predict(test_x[[sss], :])\n",
    "#         p0+=prediction[0][0]\n",
    "#         p1+=prediction[0][1]\n",
    "\n",
    "#         prediction = tcp.predict_conf(test_x[[sss], :])\n",
    "#         s0 += prediction[0][1]\n",
    "#         s1 += prediction[0][2]\n",
    "    \n",
    "#     print(round(s0) , round(s1) , round(p0-p1) , test_y[sss] , prediction[0][0])\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
